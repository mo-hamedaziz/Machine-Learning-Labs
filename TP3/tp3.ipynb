{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce2155b9-34a1-445c-8b3d-ebec6eea7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = load_iris() \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2849d03-6a1a-4cff-9ff2-96e3f3291762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88dea7fe-baf3-4e34-9590-3e56467a8419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** original dataframe ****\n",
      "   A  B  C\n",
      "0  0  1  5\n",
      "1  0  2  3\n",
      "2  0  3  4\n",
      "3  0  5  1\n",
      "4  0  4  1\n",
      "5  1  2  2\n",
      "6  1  5  3\n",
      "\n",
      "**** group by ****\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000021389337AD0>\n",
      "\n",
      "Groups and their contents:\n",
      "\n",
      "Group 1:\n",
      "   A  B  C\n",
      "0  0  1  5\n",
      "\n",
      "Group 2:\n",
      "   A  B  C\n",
      "1  0  2  3\n",
      "5  1  2  2\n",
      "\n",
      "Group 3:\n",
      "   A  B  C\n",
      "2  0  3  4\n",
      "\n",
      "Group 4:\n",
      "   A  B  C\n",
      "4  0  4  1\n",
      "\n",
      "Group 5:\n",
      "   A  B  C\n",
      "3  0  5  1\n",
      "6  1  5  3\n",
      "\n",
      "**** describe ****\n",
      "      A                                               C                      \\\n",
      "  count mean       std  min   25%  50%   75%  max count mean       std  min   \n",
      "B                                                                             \n",
      "1   1.0  0.0       NaN  0.0  0.00  0.0  0.00  0.0   1.0  5.0       NaN  5.0   \n",
      "2   2.0  0.5  0.707107  0.0  0.25  0.5  0.75  1.0   2.0  2.5  0.707107  2.0   \n",
      "3   1.0  0.0       NaN  0.0  0.00  0.0  0.00  0.0   1.0  4.0       NaN  4.0   \n",
      "4   1.0  0.0       NaN  0.0  0.00  0.0  0.00  0.0   1.0  1.0       NaN  1.0   \n",
      "5   2.0  0.5  0.707107  0.0  0.25  0.5  0.75  1.0   2.0  2.0  1.414214  1.0   \n",
      "\n",
      "                         \n",
      "    25%  50%   75%  max  \n",
      "B                        \n",
      "1  5.00  5.0  5.00  5.0  \n",
      "2  2.25  2.5  2.75  3.0  \n",
      "3  4.00  4.0  4.00  4.0  \n",
      "4  1.00  1.0  1.00  1.0  \n",
      "5  1.50  2.0  2.50  3.0  \n",
      "\n",
      "**** unstack ****\n",
      "          B\n",
      "A  count  1    1.0\n",
      "          2    2.0\n",
      "          3    1.0\n",
      "          4    1.0\n",
      "          5    2.0\n",
      "              ... \n",
      "C  max    1    5.0\n",
      "          2    3.0\n",
      "          3    4.0\n",
      "          4    1.0\n",
      "          5    3.0\n",
      "Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Nettoyage des données\n",
    "# 1.1. Visualisation des propriétés de données\n",
    "\n",
    "df = pd.DataFrame({'A': [0,0,0,0,0,1,1],\n",
    " 'B': [1,2,3,5,4,2,5],\n",
    " 'C': [5,3,4,1,1,2,3]})\n",
    "\n",
    "print('\\n**** original dataframe ****')\n",
    "print(df)\n",
    "\n",
    "print('\\n**** group by ****')\n",
    "print(df.groupby('B'))\n",
    "\n",
    "print(\"\\nGroups and their contents:\")\n",
    "for key, group in df.groupby('B'):\n",
    "    print(f\"\\nGroup {key}:\\n{group}\")\n",
    "\n",
    "print('\\n**** describe ****')\n",
    "a_group_desc = df.groupby('B').describe()\n",
    "print(a_group_desc)\n",
    "\n",
    "print('\\n**** unstack ****')\n",
    "unstacked = a_group_desc.unstack()\n",
    "print(unstacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3893c6bc-c30b-40fd-893c-c04493a66bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qst 1: Déduire le rôle des méthodes : groupby(), describe() et unstack()\n",
    "\n",
    "# groupby(): Groups the DataFrame by the unique values of column B, allowing for aggregation or descriptive statistics for each group.\n",
    "# describe(): Computes summary statistics (like count, mean, std, min, 25%, 50%, 75%, max) for each group created by groupby().\n",
    "# unstack(): Transforms the hierarchical index created by groupby() and describe() into a flat format, making the DataFrame easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c04a37b-72bc-4a2c-9d7f-0695b8d9f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame original :\n",
      "       Nom  Age      Ville\n",
      "0    Alice   25      Paris\n",
      "1      Bob   30       Lyon\n",
      "2  Charlie   35  Marseille\n",
      "3    Alice   25      Paris\n",
      "4      Bob   30       Lyon\n",
      "     Nom  Age  Ville\n",
      "3  Alice   25  Paris\n",
      "4    Bob   30   Lyon\n",
      "       Nom  Age      Ville\n",
      "0    Alice   25      Paris\n",
      "1      Bob   30       Lyon\n",
      "2  Charlie   35  Marseille\n"
     ]
    }
   ],
   "source": [
    "# 1.2. Détection et suppression des données redondantes\n",
    "\n",
    "# Exemple de jeu de données\n",
    "data = {\n",
    " 'Nom': ['Alice', 'Bob', 'Charlie', 'Alice', 'Bob'],\n",
    " 'Age': [25, 30, 35, 25, 30],\n",
    " 'Ville': ['Paris', 'Lyon', 'Marseille', 'Paris', 'Lyon']\n",
    "}\n",
    "\n",
    "# Création d'un DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Affichage du DataFrame original\n",
    "print(\"DataFrame original :\")\n",
    "print(df)\n",
    "\n",
    "db = df.duplicated()\n",
    "print(df[db])\n",
    "\n",
    "df_new = df.drop_duplicates()\n",
    "print(df_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc5c49df-ff38-45fd-a774-3d3e204d9617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Age      Ville\n",
      "0    Alice   25      Paris\n",
      "1      Bob   30       Lyon\n",
      "2  Charlie   35  Marseille\n"
     ]
    }
   ],
   "source": [
    "# Qst 2\n",
    "# a) Déduire le rôle des méthodes : duplicated() et drop_duplicates().\n",
    "    # duplicated():\n",
    "        # Identifies duplicate rows in the DataFrame.\n",
    "        # Returns a boolean Series where True indicates a duplicate row (excluding the first occurrence).\n",
    "    \n",
    "    # drop_duplicates():\n",
    "        # Removes duplicate rows from the DataFrame.\n",
    "        # By default, keeps the first occurrence of each duplicate row.\n",
    "\n",
    "# b) Supprimer les doublons en fonction de la colonne 'Nom'.\n",
    "df_no_duplicates = df.drop_duplicates(subset=['Nom'])\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c258ebf-53c7-4714-932c-28ac115116b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualiser les données manquantes : True signifie la détection d'une donnée manquante\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "dtype: bool\n",
      "Isoler les données manquantes\n",
      "3   NaN\n",
      "6   NaN\n",
      "dtype: float64\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    3.0\n",
      "4    5.0\n",
      "5    6.0\n",
      "6    3.0\n",
      "dtype: float64\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "4    5.0\n",
      "5    6.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1.3. Détection et traitement des données manquantes (missing data)\n",
    "\n",
    "# Crée une série Pandas avec des données comprenant des valeurs manquantes\n",
    "s = pd.Series([1, 2, 3, np.nan, 5, 6, None])\n",
    "\n",
    "print(\"Visualiser les données manquantes : True signifie la détection d'une donnée manquante\")\n",
    "print(s.isnull())  # Affiche une série de booléens, où True indique une donnée manquante\n",
    "\n",
    "print(\"Isoler les données manquantes\")\n",
    "print(s[s.isnull()])  # Renvoie une série contenant uniquement les données manquantes\n",
    "\n",
    "# Remplir les données manquantes avec la moyenne des valeurs existantes (après conversion en entier)\n",
    "print(s.fillna(int(s.mean())))\n",
    "\n",
    "# Supprimer les données manquantes de la série\n",
    "print(s.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e56e363-8679-4ca8-bf49-9f6fa3814fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qst 3\n",
    "\n",
    "# fillna():\n",
    "    # Fills missing values (NaN or None) in the Series with a specified value (e.g., mean, median, constant).\n",
    "    # In this case, it fills the missing values with the mean of the existing values, converted to an integer.\n",
    "\n",
    "# dropna():\n",
    "    # Removes all entries in the Series that have missing values (NaN or None).\n",
    "    # This method returns a new Series without the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a824b4e-8d17-4d63-8005-baa02c31b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data :\n",
      "       x0      x1\n",
      "0  0.3051     NaN\n",
      "1  0.4949  0.2654\n",
      "2  0.6974  0.2615\n",
      "3  0.3769  0.5846\n",
      "4  0.2231  0.4615\n",
      "5  0.3410  0.8308\n",
      "6  0.4436  0.4962\n",
      "7  0.5897  0.3269\n",
      "8  0.6308  0.5346\n",
      "9  0.5000  0.6731\n",
      "chercher les valeurs manquantes par la moyenne de la colonne\n",
      "data après imputation\n",
      "       x0        x1\n",
      "0  0.3051  0.492733\n",
      "1  0.4949  0.265400\n",
      "2  0.6974  0.261500\n",
      "3  0.3769  0.584600\n",
      "4  0.2231  0.461500\n",
      "5  0.3410  0.830800\n",
      "6  0.4436  0.496200\n",
      "7  0.5897  0.326900\n",
      "8  0.6308  0.534600\n",
      "9  0.5000  0.673100\n"
     ]
    }
   ],
   "source": [
    "# 1.4. Imputation des données manquantes\n",
    "\n",
    "# Create an empty dataset\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Create two variables called x0 and x1. Make the first value of x1 a missing value\n",
    "df['x0'] = [0.3051,0.4949,0.6974,0.3769,0.2231,0.341,0.4436,0.5897,0.6308,0.5]\n",
    "df['x1'] = [np.nan,0.2654,0.2615,0.5846,0.4615,0.8308,0.4962,0.3269,0.5346,0.6731]\n",
    "\n",
    "# View the dataset\n",
    "print(\"Data :\")\n",
    "print(df)\n",
    "\n",
    "# Create an imputer object that looks for 'Nan' values, then replaces them with the mean value of the feature\n",
    "print(\"chercher les valeurs manquantes par la moyenne de la colonne\")\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Apply the imputor on the df dataset\n",
    "mean_imputer = mean_imputer.fit(df)\n",
    "imputed_data = mean_imputer.transform(df)\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=df.columns)\n",
    "\n",
    "# View the dataset\n",
    "print(\"data après imputation\")\n",
    "print(imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bcd5492-7be3-4084-8274-24681e9cf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qst 4\n",
    "\n",
    "# Imputer is now deprecated in sklearn.preprocessing\n",
    "# We are using SimpleImputer now\n",
    "# However SimpleImputer doesn't accept an 'axis' parameter\n",
    "\n",
    "# axis — This can take one of two values — 0 and 1. This will decide if the Imputer will apply the strategy along the rows or along the columns. 0 for columns, and 1 for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eeb7910-1464-4d5b-99e5-e62c0c5d3d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C\n",
      "0  2  1  5\n",
      "1  1  2  3\n",
      "2  2  3  4\n",
      "3  3  5  1\n",
      "4  3  4  1\n",
      "5  5  2  2\n",
      "6  4  5  3\n",
      "Le jeu de données est trié selon la colonne 'A'\n",
      "   A  B  C\n",
      "0  1  2  3\n",
      "1  2  1  5\n",
      "2  2  3  4\n",
      "3  3  5  1\n",
      "4  3  4  1\n",
      "5  4  5  3\n",
      "6  5  2  2\n",
      "Le jeu de données est mélangé\n",
      "   A  B  C\n",
      "0  4  5  3\n",
      "1  3  5  1\n",
      "2  5  2  2\n",
      "3  3  4  1\n",
      "4  2  3  4\n",
      "5  1  2  3\n",
      "6  2  1  5\n"
     ]
    }
   ],
   "source": [
    "# 1.5. Sorting et Shuffling\n",
    "\n",
    "df = pd.DataFrame({'A': [2,1,2,3,3,5,4],'B': [1,2,3,5,4,2,5], 'C': [5,3,4,1,1,2,3]})\n",
    "print(df)\n",
    "\n",
    "print(\"Le jeu de données est trié selon la colonne 'A'\")\n",
    "df = df.sort_values(by=['A'], ascending=[True])\n",
    "df = df.reset_index(drop=True)\n",
    "print (df)\n",
    "\n",
    "index = df.index.tolist()\n",
    "np.random.shuffle(index)\n",
    "df = df.loc[index]\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Le jeu de données est mélangé\")\n",
    "print (df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "236c6f6f-ddd6-49ec-b459-928d358b14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qst 5\n",
    "\n",
    "# The reset_index() method resets the index of the DataFrame: It creates a new default integer index (starting from 0) and optionally drops the old index, which would otherwise become a new column in the DataFrame.\n",
    "# drop=True: Prevents the old index from being added as a column.\n",
    "\n",
    "# Example\n",
    "        # # Original DataFrame\n",
    "        # df = pd.DataFrame({'A': [2,1,2,3,3,5,4],'B': [1,2,3,5,4,2,5], 'C': [5,3,4,1,1,2,3]})\n",
    "        \n",
    "        # # Sorting the DataFrame\n",
    "        # df = df.sort_values(by=['A'], ascending=[True])\n",
    "        \n",
    "        # # Resetting the index (old index is dropped)\n",
    "        # df_reset = df.reset_index(drop=True)\n",
    "        \n",
    "        # print(df_reset)\n",
    "\n",
    "    # In this example:\n",
    "        # After sorting, the index might no longer be in sequential order.\n",
    "        # reset_index(drop=True) restores the DataFrame to have a fresh index from 0 while discarding the original index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d818674-6a65-4ef7-9b4c-ba456a942b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "    constructor          Model  MYCT  MMIN   MMAX  CACH  CHMIN  CHMAX  PRP  \\\n",
      "0       adviser          32/60   125   256   6000   256     16    128  198   \n",
      "1        amdahl         470v/7    29  8000  32000    32      8     32  269   \n",
      "2        amdahl        470v/7a    29  8000  32000    32      8     32  220   \n",
      "3        amdahl        470v/7b    29  8000  32000    32      8     32  172   \n",
      "4        amdahl        470v/7c    29  8000  16000    32      8     16  132   \n",
      "..          ...            ...   ...   ...    ...   ...    ...    ...  ...   \n",
      "204      sperry           80/8   124  1000   8000     0      1      8   42   \n",
      "205      sperry  90/80-model-3    98  1000   8000    32      2      8   46   \n",
      "206      sratus             32   125  2000   8000     0      2     14   52   \n",
      "207        wang         vs-100   480   512   8000    32      0      0   67   \n",
      "208        wang          vs-90   480  1000   4000     0      0      0   45   \n",
      "\n",
      "     ERP  \n",
      "0    199  \n",
      "1    253  \n",
      "2    253  \n",
      "3    253  \n",
      "4    132  \n",
      "..   ...  \n",
      "204   37  \n",
      "205   50  \n",
      "206   41  \n",
      "207   47  \n",
      "208   25  \n",
      "\n",
      "[209 rows x 10 columns]\n",
      "\n",
      "********** Normalisation*********\n",
      "\n",
      "Moyenne apres le Min max Scaling :\n",
      "MYCT=0.13, MMAX=0.18\n",
      "\n",
      "\n",
      "Valeur minimale et maximale pour la feature MYCT apres min max scaling: \n",
      "MIN=0.00, MAX=1.00\n",
      "\n",
      "\n",
      "Valeur minimale et maximale pour la feature MMAX apres min max scaling : \n",
      "MIN=0.00, MAX=1.00\n",
      "data après normalisation\n",
      "[[0.07282535 0.09284284]\n",
      " [0.00809171 0.4994995 ]\n",
      " [0.00809171 0.4994995 ]\n",
      " [0.00809171 0.4994995 ]\n",
      " [0.00809171 0.24924925]\n",
      " [0.00606878 0.4994995 ]\n",
      " [0.00404585 0.4994995 ]\n",
      " [0.00404585 0.4994995 ]\n",
      " [0.00404585 1.        ]\n",
      " [0.00404585 1.        ]\n",
      " [0.25826028 0.04592092]\n",
      " [0.25826028 0.05374124]\n",
      " [0.02899528 0.12412412]\n",
      " [0.02225219 0.24924925]\n",
      " [0.22454484 0.        ]\n",
      " [0.12339852 0.24924925]\n",
      " [0.10114633 0.03028028]\n",
      " [0.08496291 0.0772022 ]\n",
      " [0.08496291 0.03028028]\n",
      " [0.06271072 0.0772022 ]\n",
      " [0.08496291 0.09753504]\n",
      " [0.08496291 0.09597097]\n",
      " [0.08496291 0.09597097]\n",
      " [0.06271072 0.09597097]\n",
      " [0.20431558 0.09284284]\n",
      " [0.20431558 0.03028028]\n",
      " [0.20431558 0.09284284]\n",
      " [0.20431558 0.04592092]\n",
      " [0.20431558 0.0772022 ]\n",
      " [0.20431558 0.0772022 ]\n",
      " [0.00539447 0.03997748]\n",
      " [0.00539447 0.03997748]\n",
      " [0.02225219 0.16291291]\n",
      " [0.02225219 0.16291291]\n",
      " [0.02629804 0.32698323]\n",
      " [0.03169252 0.32698323]\n",
      " [0.02225219 0.03028028]\n",
      " [0.02225219 0.06156156]\n",
      " [0.02225219 0.12412412]\n",
      " [0.02225219 0.06156156]\n",
      " [0.02225219 0.12412412]\n",
      " [0.02225219 0.24924925]\n",
      " [0.02225219 0.24924925]\n",
      " [0.02225219 0.24924925]\n",
      " [0.07821982 0.18668669]\n",
      " [0.07821982 0.12412412]\n",
      " [0.5347269  0.00700701]\n",
      " [0.5347269  0.0772022 ]\n",
      " [0.20431558 0.12412412]\n",
      " [0.12339852 0.12412412]\n",
      " [0.46055293 0.12412412]\n",
      " [0.46055293 0.03028028]\n",
      " [0.08293999 0.24924925]\n",
      " [0.12339852 0.12412412]\n",
      " [0.06271072 0.06156156]\n",
      " [0.06271072 0.18668669]\n",
      " [0.13688469 0.12412412]\n",
      " [0.52798382 0.12412412]\n",
      " [0.52798382 0.12412412]\n",
      " [0.52798382 0.12412412]\n",
      " [0.52798382 0.12412412]\n",
      " [0.52798382 0.12412412]\n",
      " [0.07282535 0.01463964]\n",
      " [0.03910991 0.12412412]\n",
      " [0.03910991 0.24924925]\n",
      " [0.03910991 0.24924925]\n",
      " [0.04922454 0.01463964]\n",
      " [0.05933918 0.03028028]\n",
      " [0.05933918 0.06156156]\n",
      " [0.05933918 0.06156156]\n",
      " [0.03910991 0.12412412]\n",
      " [0.03910991 0.12412412]\n",
      " [0.1065408  0.03028028]\n",
      " [0.1908294  0.04592092]\n",
      " [0.1908294  0.04592092]\n",
      " [0.1908294  0.18668669]\n",
      " [0.1908294  0.06938188]\n",
      " [0.1908294  0.18668669]\n",
      " [0.1908294  0.01101101]\n",
      " [0.10991234 0.18668669]\n",
      " [0.21105866 0.04592092]\n",
      " [0.1908294  0.06156156]\n",
      " [0.1908294  0.24924925]\n",
      " [0.21105866 0.03028028]\n",
      " [0.21105866 0.06156156]\n",
      " [0.08293999 0.06156156]\n",
      " [0.08293999 0.06156156]\n",
      " [0.08293999 0.06156156]\n",
      " [0.08293999 0.4994995 ]\n",
      " [0.08293999 0.12412412]\n",
      " [0.08293999 0.4994995 ]\n",
      " [0.08293999 0.4994995 ]\n",
      " [0.08293999 0.06156156]\n",
      " [0.02697235 0.24924925]\n",
      " [0.02697235 0.37437437]\n",
      " [0.00606878 0.4994995 ]\n",
      " [0.00606878 0.4994995 ]\n",
      " [0.00606878 0.4994995 ]\n",
      " [0.00606878 0.24924925]\n",
      " [0.31220499 0.00700701]\n",
      " [0.12542144 0.03028028]\n",
      " [0.06608227 0.09284284]\n",
      " [0.73027647 0.02245996]\n",
      " [0.73027647 0.03028028]\n",
      " [0.39312205 0.03028028]\n",
      " [0.25826028 0.06156156]\n",
      " [0.25826028 0.12412412]\n",
      " [0.5954147  0.01463964]\n",
      " [0.5954147  0.01463964]\n",
      " [0.5954147  0.06156156]\n",
      " [0.5954147  0.06156156]\n",
      " [0.5954147  0.06156156]\n",
      " [0.14025624 0.06156156]\n",
      " [0.14025624 0.06156156]\n",
      " [0.10991234 0.12412412]\n",
      " [0.11328388 0.24924925]\n",
      " [0.10991234 0.24924925]\n",
      " [0.14025624 0.06156156]\n",
      " [0.00539447 0.18668669]\n",
      " [0.00539447 0.18668669]\n",
      " [0.         0.24924925]\n",
      " [0.         0.24924925]\n",
      " [1.         0.01463964]\n",
      " [1.         0.03028028]\n",
      " [0.52798382 0.03028028]\n",
      " [0.02225219 0.06156156]\n",
      " [0.02225219 0.12412412]\n",
      " [0.02225219 0.12412412]\n",
      " [0.02225219 0.24924925]\n",
      " [0.02225219 0.24924925]\n",
      " [0.02225219 0.24924925]\n",
      " [0.05596763 0.12412412]\n",
      " [0.05596763 0.12412412]\n",
      " [0.05596763 0.12412412]\n",
      " [0.02225219 0.24924925]\n",
      " [0.02225219 0.24924925]\n",
      " [0.02225219 0.24924925]\n",
      " [0.08968307 0.06156156]\n",
      " [0.06608227 0.12412412]\n",
      " [0.06608227 0.06156156]\n",
      " [0.05057316 0.12412412]\n",
      " [0.05057316 0.12412412]\n",
      " [0.05057316 0.12412412]\n",
      " [0.03910991 0.24924925]\n",
      " [0.02899528 0.24924925]\n",
      " [0.02899528 0.24924925]\n",
      " [0.02899528 0.24924925]\n",
      " [0.02225219 0.24924925]\n",
      " [0.03708699 0.24924925]\n",
      " [0.03708699 0.12412412]\n",
      " [0.0155091  0.24924925]\n",
      " [0.0155091  0.4994995 ]\n",
      " [0.01213756 0.4994995 ]\n",
      " [0.01416049 0.4994995 ]\n",
      " [0.02090357 0.37437437]\n",
      " [0.01416049 0.4994995 ]\n",
      " [0.00876601 0.4994995 ]\n",
      " [0.06405934 0.01463964]\n",
      " [0.04517869 0.03028028]\n",
      " [0.02629804 0.06156156]\n",
      " [0.02629804 0.09284284]\n",
      " [0.02629804 0.12412412]\n",
      " [0.02629804 0.12412412]\n",
      " [0.02629804 0.18668669]\n",
      " [0.02629804 0.24924925]\n",
      " [0.01416049 0.12412412]\n",
      " [0.01416049 0.12412412]\n",
      " [0.01416049 0.24924925]\n",
      " [0.01416049 0.37437437]\n",
      " [0.01416049 0.24924925]\n",
      " [0.12339852 0.03028028]\n",
      " [0.12339852 0.06156156]\n",
      " [0.12339852 0.12412412]\n",
      " [0.15711396 0.06156156]\n",
      " [0.15711396 0.06156156]\n",
      " [0.15711396 0.24924925]\n",
      " [0.09642616 0.06156156]\n",
      " [0.09642616 0.03028028]\n",
      " [0.09642616 0.06156156]\n",
      " [0.09642616 0.12412412]\n",
      " [0.09642616 0.12412412]\n",
      " [0.15037087 0.01463964]\n",
      " [0.15037087 0.03028028]\n",
      " [0.05933918 0.06156156]\n",
      " [0.05933918 0.09284284]\n",
      " [0.05933918 0.12412412]\n",
      " [0.02360081 0.24924925]\n",
      " [0.03573837 0.18668669]\n",
      " [0.02832097 0.18668669]\n",
      " [0.02832097 0.24924925]\n",
      " [0.00606878 0.37437437]\n",
      " [0.00606878 0.4994995 ]\n",
      " [0.00606878 0.4994995 ]\n",
      " [0.06675657 0.12412412]\n",
      " [0.02225219 0.4994995 ]\n",
      " [0.02225219 0.4994995 ]\n",
      " [0.02225219 0.4994995 ]\n",
      " [0.02225219 0.4994995 ]\n",
      " [0.00876601 1.        ]\n",
      " [0.00876601 1.        ]\n",
      " [0.10991234 0.06156156]\n",
      " [0.10991234 0.06156156]\n",
      " [0.10991234 0.06156156]\n",
      " [0.10991234 0.06156156]\n",
      " [0.07215105 0.12412412]\n",
      " [0.05461902 0.12412412]\n",
      " [0.07282535 0.12412412]\n",
      " [0.31220499 0.12412412]\n",
      " [0.31220499 0.06156156]]\n"
     ]
    }
   ],
   "source": [
    "# 2. Transformation des données (Normalisation)\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data'\n",
    "names= ['constructor','Model','MYCT','MMIN','MMAX','CACH','CHMIN','CHMAX','PRP','ERP']\n",
    "\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "print(\"data:\")\n",
    "print(dataset)\n",
    "\n",
    "# MIN MAX SCALING\n",
    "minmax_scale = MinMaxScaler().fit(dataset[['MYCT', 'MMAX']])\n",
    "df_minmax = minmax_scale.transform(dataset[['MYCT', 'MMAX']])\n",
    "\n",
    "print('\\n********** Normalisation*********\\n')\n",
    "\n",
    "print('Moyenne apres le Min max Scaling :\\nMYCT={:.2f}, MMAX={:.2f}'\n",
    ".format(df_minmax[:,0].mean(), df_minmax[:,1].mean()))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Valeur minimale et maximale pour la feature MYCT apres min max scaling: \\nMIN={:.2f}, MAX={:.2f}'.format(df_minmax[:,0].min(), df_minmax[:,0].max()))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Valeur minimale et maximale pour la feature MMAX apres min max scaling : \\nMIN={:.2f}, MAX={:.2f}'.format(df_minmax[:,1].min(), df_minmax[:,1].max()))\n",
    "print(\"data après normalisation\")\n",
    "print(df_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b45a0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we used the MinMaxScaler to normalize the 'MYCT' \n",
    "# and 'MMAX' columns in the dataset, after normalizing the data, the \n",
    "# minimum and maximum values of the 'MYCT' and 'MMAX' columns are\n",
    "# 0.0 and 1.0, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c30ca7eb-6755-4744-b1ca-6065a6eb868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Map  Values\n",
      "0    0       1\n",
      "1    0       2\n",
      "2    0       3\n",
      "3    1       5\n",
      "4    1       4\n",
      "5    2       2\n",
      "6    2       5\n",
      "   Map  Values  Sum  Moy\n",
      "0    0       1    6  2.0\n",
      "1    0       2    6  2.0\n",
      "2    0       3    6  2.0\n",
      "3    1       5    9  4.5\n",
      "4    1       4    9  4.5\n",
      "5    2       2    7  3.5\n",
      "6    2       5    7  3.5\n"
     ]
    }
   ],
   "source": [
    "# 3. Reduction des données (Agrégation des données)\n",
    "\n",
    "df = pd.DataFrame({'Map': [0,0,0,1,1,2,2], 'Values': [1,2,3,5,4,2,5]})\n",
    "print(df)\n",
    "\n",
    "df['Sum'] = df.groupby('Map')['Values'].transform('sum')\n",
    "df['Moy'] = df.groupby('Map')['Values'].transform('mean')\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6ccc65b-2911-41ba-b1e1-bf3a98e77ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qst 6\n",
    "\n",
    "# The Map column acts as a group identifier, dividing the rows into groups (e.g., 0, 1, 2).\n",
    "# Operations like groupby('Map') calculate statistics (sum, mean, etc ...) separately for each group and apply the results to the corresponding rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a4e5075-a310-4c1b-b3cf-cecdda064872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRIS data values:\n",
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "         group  \n",
      "0       setosa  \n",
      "1       setosa  \n",
      "2       setosa  \n",
      "3       setosa  \n",
      "4       setosa  \n",
      "..         ...  \n",
      "145  virginica  \n",
      "146  virginica  \n",
      "147  virginica  \n",
      "148  virginica  \n",
      "149  virginica  \n",
      "\n",
      "[150 rows x 5 columns]\n",
      "Mean value :\n",
      "sepal length (cm)    5.843333\n",
      "sepal width (cm)     3.057333\n",
      "petal length (cm)    3.758000\n",
      "petal width (cm)     1.199333\n",
      "dtype: float64\n",
      "Median value :\n",
      "sepal length (cm)    5.80\n",
      "sepal width (cm)     3.00\n",
      "petal length (cm)    4.35\n",
      "petal width (cm)     1.30\n",
      "dtype: float64\n",
      "Discrétisation basée sur des effectifs égaux (ou quantiles) :\n",
      "      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0.00                4.3               2.0               1.00               0.1\n",
      "0.25                5.1               2.8               1.60               0.3\n",
      "0.50                5.8               3.0               4.35               1.3\n",
      "0.75                6.4               3.3               5.10               1.8\n",
      "1.00                7.9               4.4               6.90               2.5\n",
      "Bining IrisData\n",
      "             sepal length (cm) sepal width (cm) petal length (cm)  \\\n",
      "0    (4.2989999999999995, 5.1]       (3.3, 4.4]      (0.999, 1.6]   \n",
      "1    (4.2989999999999995, 5.1]       (2.8, 3.0]      (0.999, 1.6]   \n",
      "2    (4.2989999999999995, 5.1]       (3.0, 3.3]      (0.999, 1.6]   \n",
      "3    (4.2989999999999995, 5.1]       (3.0, 3.3]      (0.999, 1.6]   \n",
      "4    (4.2989999999999995, 5.1]       (3.3, 4.4]      (0.999, 1.6]   \n",
      "..                         ...              ...               ...   \n",
      "145                 (6.4, 7.9]       (2.8, 3.0]        (5.1, 6.9]   \n",
      "146                 (5.8, 6.4]     (1.999, 2.8]       (4.35, 5.1]   \n",
      "147                 (6.4, 7.9]       (2.8, 3.0]        (5.1, 6.9]   \n",
      "148                 (5.8, 6.4]       (3.3, 4.4]        (5.1, 6.9]   \n",
      "149                 (5.8, 6.4]       (2.8, 3.0]       (4.35, 5.1]   \n",
      "\n",
      "    petal width (cm)  \n",
      "0       (0.099, 0.3]  \n",
      "1       (0.099, 0.3]  \n",
      "2       (0.099, 0.3]  \n",
      "3       (0.099, 0.3]  \n",
      "4       (0.099, 0.3]  \n",
      "..               ...  \n",
      "145       (1.8, 2.5]  \n",
      "146       (1.8, 2.5]  \n",
      "147       (1.8, 2.5]  \n",
      "148       (1.8, 2.5]  \n",
      "149       (1.3, 1.8]  \n",
      "\n",
      "[150 rows x 4 columns]\n",
      "Fréquence dans chanque catégorie\n",
      "group\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "Fréquence pour chaque marge de valeurs\n",
      "petal length (cm)\n",
      "(0.999, 1.6]    44\n",
      "(4.35, 5.1]     41\n",
      "(5.1, 6.9]      34\n",
      "(1.6, 4.35]     31\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4. Discrétisation des données\n",
    "\n",
    "# Convertir les données Iris en un tableau numpy et un DataFrame pandas\n",
    "iris_nparray = iris.data\n",
    "iris_dataframe = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Ajouter une colonne catégorique 'group'\n",
    "iris_dataframe['group'] = pd.Series([iris.target_names[k] for k in iris.target], dtype=\"category\")\n",
    "\n",
    "print(\"IRIS data values:\")\n",
    "print(iris_dataframe)\n",
    "\n",
    "print(\"Mean value :\")\n",
    "print (iris_dataframe.mean(numeric_only=True))\n",
    "\n",
    "print(\"Median value :\")\n",
    "print (iris_dataframe.median(numeric_only=True))\n",
    "\n",
    "# Sélectionner uniquement les colonnes numériques\n",
    "numerical_columns = iris_dataframe.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculer et afficher les quantiles (0%, 25%, 50%, 75%, 100%) pour les colonnes numériques\n",
    "print(\"Discrétisation basée sur des effectifs égaux (ou quantiles) :\")\n",
    "print(numerical_columns.quantile(np.array([0,.25,.50,.75,1])))\n",
    "\n",
    "#Le binning transforme les variables numériques en variables catégoriques (discrétisation) pour chaque colonne numérique\n",
    "iris_binned = pd.concat([\n",
    " pd.qcut(iris_dataframe.iloc[:,0], [0, .25, .5, .75, 1]),\n",
    " pd.qcut(iris_dataframe.iloc[:,1], [0, .25, .5, .75, 1]),\n",
    " pd.qcut(iris_dataframe.iloc[:,2], [0, .25, .5, .75, 1]),\n",
    " pd.qcut(iris_dataframe.iloc[:,3], [0, .25, .5, .75, 1]),\n",
    " ], join='outer', axis = 1)\n",
    "\n",
    "print(\"Bining IrisData\")\n",
    "print(iris_binned)\n",
    "\n",
    "# Calculer et afficher la fréquence pour chaque catégorie dans la colonne 'group'\n",
    "print(\"Fréquence dans chanque catégorie\")\n",
    "print (iris_dataframe['group'].value_counts())\n",
    "\n",
    "# Calculer et afficher la fréquence pour chaque intervalle de valeurs discrétisées dans la colonne 'petal length (cm)'\n",
    "print(\"Fréquence pour chaque marge de valeurs\")\n",
    "print (iris_binned['petal length (cm)'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b557fde5-605a-45d9-9d70-c6d47e7acf3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Atelier_ML\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:565\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m     arr \u001b[38;5;241m=\u001b[39m sanitize_array(data, \u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Atelier_ML\\Lib\\site-packages\\pandas\\core\\construction.py:659\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    657\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[1;32m--> 659\u001b[0m subarr \u001b[38;5;241m=\u001b[39m _sanitize_ndim(subarr, data, dtype, index, allow_2d\u001b[38;5;241m=\u001b[39mallow_2d)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Atelier_ML\\Lib\\site-packages\\pandas\\core\\construction.py:718\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[1;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m     )\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (5, 4) instead",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Qst 7\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# qcut() discretizes continuous data into categories (bins) based on quantiles. \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Yes, but only if equal-sized intervals (by value range) are acceptable. pd.cut divides data into fixed-width bins, not based on equal counts.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# To replace qcut with cut\u001b[39;00m\n\u001b[0;32m     13\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m numerical_columns\u001b[38;5;241m.\u001b[39mquantile([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 14\u001b[0m binned_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(numerical_columns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal length (cm)\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39mquantiles, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Atelier_ML\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:253\u001b[0m, in \u001b[0;36mcut\u001b[1;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverlapping IntervalIndex is not accepted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 253\u001b[0m     bins \u001b[38;5;241m=\u001b[39m Index(bins)\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bins\u001b[38;5;241m.\u001b[39mis_monotonic_increasing:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbins must increase monotonically.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Atelier_ML\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:528\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_scalar_data_error(data)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(np\u001b[38;5;241m.\u001b[39masarray(data), dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mmemoryview\u001b[39m):\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;66;03m# 2022-11-16 the memoryview check is only necessary on some CI\u001b[39;00m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;66;03m#  builds, not clear why\u001b[39;00m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_scalar_data_error(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Atelier_ML\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:570\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_scalar_data_error(data) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[1;32m--> 570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex data must be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    572\u001b[0m arr \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(arr)\n",
      "\u001b[1;31mValueError\u001b[0m: Index data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# Qst 7\n",
    "\n",
    "# qcut() discretizes continuous data into categories (bins) based on quantiles. \n",
    "# It divides the data into bins of equal observations.\n",
    "# t’s useful when you want balanced groups.\n",
    "\n",
    "# We can use cut() but it divides the data into equal-width intervals\n",
    "# based on the range of the data, regardless of how many data points fall into each bin.\n",
    "\n",
    "# Another alternative\n",
    "# Yes, but only if equal-sized intervals (by value range) are acceptable. pd.cut divides data into fixed-width bins, not based on equal counts.\n",
    "# To replace qcut with cut\n",
    "quantiles = numerical_columns.quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "binned_data = pd.cut(numerical_columns['petal length (cm)'], bins=quantiles, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5d2a3-545a-4eeb-be92-9b5f5ee6a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Réduction de dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae29a3-c48d-472f-be53-4d1abce605d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Réduction basée sur une sélection des caractéristiques (Méthode Filter)\n",
    "\n",
    "X,y = iris.data, iris.target # X = données des features, y = cibles (classes)\n",
    "\n",
    "# Calcul des scores de corrélation pour sélectionner les features les plus pertinentes\n",
    "chi2_score, chi_2_p_value = chi2(X,y)  # Test Chi-square\n",
    "f_score, f_p_value = f_classif(X,y) # Test ANOVA F-score\n",
    "mut_info_score = mutual_info_classif(X,y) # Information mutuelle pour mesurer la dépendance\n",
    "\n",
    "features = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "# Création d'un DataFrame pour afficher les scores calculés\n",
    "print(\"Les scores de corrélation des features :\")\n",
    "scores_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Chi-Square': chi2_score,\n",
    "    'F-Score': f_score,\n",
    "    'Mutual Information': mut_info_score\n",
    "})\n",
    "print(scores_df)\n",
    "\n",
    "# Calcul de la matrice de corrélation des features\n",
    "dataframe = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "corr = dataframe.corr() # Corrélations entre les colonnes (features)\n",
    "\n",
    "# Visualisation de la matrice de corrélation avec un heatmap\n",
    "sns.heatmap(corr,\n",
    " xticklabels=corr.columns.values,\n",
    " yticklabels=corr.columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e0c22d-93d8-49e7-a2e3-4c8f88566320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qst 8\n",
    "\n",
    "# Interpretation of the scores\n",
    "    # Petal Length and Petal Width have high scores in all three methods:\n",
    "        # Chi-Square: Measures the dependence between the feature and the target.\n",
    "            # Petal Length and Petal Width have high scores meaning they are strongly\n",
    "            # related with the target et vice versa.\n",
    "        # F-Score: Measures the correlation between the feature and the target.\n",
    "            # Petal Length and Petal Width (with the highest scores) are the most\n",
    "            # correlated with the target (best discriminators).\n",
    "        # Mutual Information: Measures the amount of information shared between \n",
    "            # the feature and the target.\n",
    "            # Petal Length and Petal Width are highly informative about the species \n",
    "            # and strongly help in distinguishing between the iris classes.\n",
    "            # Sepal Width (0.24) is the least informative feature.\n",
    "\n",
    "# Interpretation of the heatmap\n",
    "    # The heatmap shows the correlation between the features in the iris dataset.\n",
    "    # Petal Length and Petal Width have a high positive correlation (0.9) \n",
    "    # while sepal features show no correlations with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7e5a3-6c78-4968-ad7d-ec8700830b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Réduction basée sur une transformation des données (PCA)\n",
    "\n",
    "# Charger un jeu de données Iris\n",
    "data = load_iris()\n",
    "X = data.data # Caractéristiques\n",
    "y = data.target # Étiquettes\n",
    "\n",
    "# Étape 1 : Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Étape 2 : Appliquer la PCA\n",
    "pca = PCA(n_components=2) # Choisir le nombre de composantes principales\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Étape 3 : Afficher la variance expliquée par chaque composante\n",
    "print(\"Variance expliquée par chaque composante :\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Étape 4 : Convertir en DataFrame pour visualiser les nouvelles caractéristiques\n",
    "df_pca = pd.DataFrame(X_pca, columns=['Composante 1', 'Composante 2'])\n",
    "df_pca['Classe'] = y\n",
    "print(df_pca.head())\n",
    "\n",
    "#plot graphique d’observation des données\n",
    "with plt.style.context('seaborn-v0_8-whitegrid'):\n",
    " plt.figure(figsize=(6, 4))\n",
    " for lab, col in zip((0, 1, 2), ('blue', 'red', 'green')):\n",
    "     plt.scatter(X_pca[y==lab, 0],\n",
    "     X_pca[y==lab, 1],\n",
    "     label=lab, c=col)\n",
    "     plt.xlabel('Principal Component 1')\n",
    "     plt.ylabel('Principal Component 2')\n",
    "     plt.legend(loc='lower center')\n",
    "     plt.tight_layout()\n",
    "     plt.show()\n",
    "\n",
    "with plt.style.context('seaborn-v0_8-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4)) \n",
    "    for lab, col in zip((0, 1, 2), ('blue', 'red', 'green')):\n",
    "        plt.scatter(X_pca[y == lab, 0], \n",
    "                    X_pca[y == lab, 1],\n",
    "                    label=f'Classe {lab}', \n",
    "                    c=col) \n",
    "    plt.xlabel('Composante Principale 1')\n",
    "    plt.ylabel('Composante Principale 2')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Gaphique de la variance expliquée cumulative\n",
    "pca_full = PCA().fit(X_scaled)\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), pca_full.explained_variance_ratio_.cumsum(), marker='o')\n",
    "plt.xlabel('Nombre de Composantes')\n",
    "plt.ylabel('Variance expliquée cumulée')\n",
    "plt.title('Sélection du nombre de composantes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0a666-1d40-4e00-9134-ac4dbf1baa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance expliquée par chaque composante : [0.72962445 0.22850762]\n",
    "# La première composante explique 73% de la variance totale, \n",
    "# tandis que la deuxième composante explique 23% ce qui donne un total de 96%\n",
    "# d'où la réduction de la dimensionnalité avec maximum d'information.\n",
    "\n",
    "# La classe 0 (bleue) est bien séparée des classes 1 et 2 (rouge et verte)\n",
    "# qui se chevauchent légèrement. Cela montre que la PCA a bien réduit la\n",
    "# dimensionnalité tout en conservant les informations importantes pour la\n",
    "# classification.\n",
    "\n",
    "# Le graphe de la variance expliquée cumulée aide à déterminer combien de \n",
    "# composantes sont nécessaires pour expliquer une part donnée de la variance totale\n",
    "# ce qui est égal à deux dans notre cas (96%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
